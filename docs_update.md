# Why Buildpacks

Status qou was that:

users of the platform can have their projects processed as Docker images.
This currently requires a Conda environment and an environment.yml as well as a Dockerfile.
To enable more flexibility, pip and poetry will also be supported. In addition, the process can be serialized and simplified with buildpacks.
The users then no longer need a docker file and can continue to use their resources for our platform. With the buildpacks, we can set up the platform more broadly.
They are variable and can be extended to languages other than Python if required.


## How to build a Builder

A builder.toml is required to create a Builder. All information required for the necessary buildpacks must be in the correct order in the .toml.
A buildpack image is then generated from this. [builder.toml docs](https://buildpacks.io/docs/reference/config/builder-config/) 

With the cli (pack) provided by Cloud Native, the buildpack can then be created from this .toml.  konkretisieren


This builder image then generates OCI images from an app template.
At the moment the Python and the Procfile buildpack from Paketo are used for the necessary requirements.
A profile is required to be able to determine the container's entry point in the future. 


## Development and Prerequisites

In the first instance, the team opted for the pack CLI as a tool. 
To create a builder, a builder.toml must be created.
[builder.toml docs](https://buildpacks.io/docs/reference/config/builder-config/)
The builpacks to be used are written in the .toml. 
First, the Python and Procfile buildpacks from Paketo Buildpacks are used.
With this setup all Python projects can be converted into docker images. 
To execute a user job, the project is copied into a serverless template.
The template contains a run method and all necessary implementations to process the data of the job.
This run method calls the user code. This is necessary to create as few barriers as possible for the user and to minimize sources of error.  

```bash
 job-template/
  ├── app/
  │    ├──user-code
  │    └──...
  ├── tests/
  └── ...
 ```

In the previous setup, the requirements are stored in the user code directory.
In future, these must be saved in the app directory.
The builder searches the app directory for all requirements to determine the environment of the container.
During local testing and implementation, I added them manually.

## PlanQK Platform

The requirements must be copied from the user code directory to the app directory.
It must be tested that there is only one requirements.txt or one environment.yml.
The Python buildpack will always recognize a Pip environment if it finds a requirements.txt.
At the end, the docker image and the container should be deleted again. 
If a Docker -prune is planned, the builder must be created again afterwards. 
The builder itself is also available as a docker image.

## Kubernetes (Docker Runtime)

However, the use of buildpacks results in changes. The requirements were previously stored in the user-code directory. 
These must now be saved at the job-template directory. 
Instead of the Dockerfile, a Procfile is now required to start the app.
 When building the container, 

the docker engine only

 looks at the root directory of the image for necessary environmental conditions. Procfile and all requirements must be saved here. It is no longer possible to have a requirements.txt and an environment.yml in the same directory.  
 This is because the buildpack will always use pip first. 
 If a conda environment is desired, no requirements.txt must be present. 




# Creating and using a Python builder - an example

Cloud native buildpacks provide their own CLI. This can be obtained from their website: [pack CLI](https://buildpacks.io/docs/tools/pack/#install).  
The pack CLI and Docker are required for the example process. 

More information can be read on the [website](https://buildpacks.io/docs/concepts/) from CLoud Native Buildpacks.

To build a builder, the Docker engine must be running. 

```bash

git clone https://github.com/PlanQK/planqk-buildpacks
cd planqk-buildpacks\planqk-base
pack builder create planqk-base-builder --config .\builder.toml

```
A Python builder has now been created.  
There is a test app to test whether the builder works. 

```bash
cd..
cd conda/sample
pack build my_testapp --builder planqk-base-builder

```
The builder has built an OCI image from the app. Docker is required to start it. 

```bash
 docker run -it -e PORT=8080 -p 8080:8080 my_testapp
```
Localhost:8080 can now be opened in the browser.




# starting a job with buildpacks

```bash
git clone https://gitlab.com/StoneOne/planqk/serverless-template.git
cd serverless-template/job-template/app
planqk init
mv <directory-name> user_code # The directory name is the name randomly generated by planqk init
cd..
```

* windows users replace 'mv' with 'move'
The current directory should now be the job-template directory

```bash
cp app/user-code/requirements.txt .
touch Procfile
nano Procfile
web: python -m app
'Crtl + 0'
'Crtl + X'
```

windows user follow the following commands

```bash
copy app\user-code\requirements.txt .
echo web: python -m app > Procfile
notepad Procfile
```

There must only be a requirements.txt OR an environment.yml in the directory. If present, the environment.yml must be deleted. The docker engine must also be running.
Now all preparations are completed and the template can be built into an image.

```bash
pack build user-service --builder planqk-base-builder
```

### start the docker container

In case, you do not use any input data or parameters that need to be passed into the container, you may run the container with the following command:

```bash
docker run -it \
  -e BASE64_ENCODED=false \
  -e LOG_LEVEL=DEBUG \
  user-service
 ```

However, to pass the "data" and "params" attributes as JSON-serialized files into the container, you either mount it in the form of separate files (recommended) or pass it as environment variables (base64 encoded).

To use the [`data.json`](input/data.json) and [`params.json`](input/params.json) from the [`input`](input) directory, you could execute the following command:

```bash 
cd app
cd user_code
```

```bash
PROJECT_ROOT=(`pwd`) 
docker run -it \
  -e BASE64_ENCODED=false \
  -e LOG_LEVEL=DEBUG \
  -v $PROJECT_ROOT/input:/var/input \
  user-service
```


* problems : As the build process is started at job-template level, the requirements must also be saved there. However, these come from the user code and must then be moved from there. This makes an additional implementation necessary.


