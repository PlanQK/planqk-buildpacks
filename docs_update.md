# Why Buildpacks

Status qou was that:

users of the platform can have their projects processed as Docker images.
This currently requires a Conda environment and an environment.yml as well as a Dockerfile.
To enable more flexibility, pip and poetry will also be supported.
In addition, the process can be serialized and simplified with Buildpacks.
With the Buildpacks, we can set up the platform more broadly.
They are variable and can be extended to languages other than Python if required.
For more information about Buildpacks go to [Buildpacks website](https://buildpacks.io/).

## Development and Prerequisites

In the first instance, the team opted for the pack CLI as a tool. 
To create a Builder, a builder.toml must be created.
You can watch [builder.toml docs](https://buildpacks.io/docs/reference/config/builder-config/) for more information.
The Builpacks to be used are written in the builder.toml. 
First, the Python and Procfile Buildpacks from Paketo Buildpacks are used.
The procfile is a way of defining the entry point for the application.
At the moment this should always be the app directory.
An example of how to create a Builder and how to use it follows below.
With this setup all Python projects can be converted into docker images.
The procfile has been added to the job-template directory.
To execute a job/service, the user code is copied into a template directory.
The template contains a run method and all necessary implementations to process the data of the job.
This run method calls the user code. This is necessary to create as few barriers as possible for the user and to minimize sources of error.  

```bash
 job-template/
  ├── app/
  │    ├──user_code
  │    └──...
  ├── tests/
  └── ...
 ```

In the previous setup, the requirements are stored in the user code directory.
In future, these must be saved in the job-template directory.
The Builder searches the app directory for all requirements to determine the environment of the container.
During local testing and implementation, I added them manually.
With this first prototype I was able to run services from [qAI directory](https://gitlab.com/StoneOne/planqk/qai) and planqk-init locally.

## PlanQK Platform

The platform offers the possibility to upload the user code as a zip file.

## Kubernetes (Docker Runtime)

The Builder image must be saved as a persistent volume.
This ensures that pods are fail-safe.

Run the Docker container
* Docker run:
```bash
PROJECT_ROOT=(`pwd`) 
docker run -it \
  -e BASE64_ENCODED=false \
  -v $PROJECT_ROOT/user-code-template/input/data.json:/var/input/data/data.json \
  -v $PROJECT_ROOT/user-code-template/input/params.json:/var/input/params/params.json \
```


## What's still in progress
However, the use of buildpacks results in changes. The requirements were previously stored in the user _code directory. 
These must now be saved at the job-template directory. 
It is no longer possible to have a requirements.txt and an environment.yml in the same directory.  
This is because the buildpack will always use pip first. 
If a conda environment is desired, no requirements.txt must be present. 




# Creating and using a Python builder - an example

Cloud native buildpacks provide their own CLI. This can be obtained from their website: [pack CLI](https://buildpacks.io/docs/tools/pack/#install).  
The pack CLI and Docker are required for the example process. 

More information can be read on the [website](https://buildpacks.io/docs/concepts/) from CLoud Native Buildpacks.

To build a builder, the Docker engine must be running. 

```bash

git clone https://github.com/PlanQK/planqk-buildpacks
cd planqk-buildpacks\planqk-base
pack builder create planqk-base-builder --config .\builder.toml

```
A Python builder has now been created.  
There is a test app to test whether the builder works. 

```bash
cd..
cd conda/sample
pack build my_testapp --builder planqk-base-builder

```
The builder has built an OCI image from the app. Docker is required to start it. 

```bash
 docker run -it -e PORT=8080 -p 8080:8080 my_testapp
```
Localhost:8080 can now be opened in the browser.




# starting a job with buildpacks

```bash
git clone https://gitlab.com/StoneOne/planqk/serverless-template.git
cd serverless-template/job-template/app
planqk init
mv <directory-name> user_code # The directory name is the name randomly generated by planqk init
cd..
```

* windows users replace 'mv' with 'move'
The current directory should now be the job-template directory

```bash
cp app/user_code/requirements.txt .
```

windows user follow the following commands

```bash
copy app\user_code\requirements.txt .
```

There must only be a requirements.txt OR an environment.yml in the directory. If present, the environment.yml must be deleted. The docker engine must also be running.
Now all preparations are completed and the template can be built into an image.

```bash
pack build user-service --builder planqk-base-builder
```

### start the docker container

In case, you do not use any input data or parameters that need to be passed into the container, you may run the container with the following command:

```bash
docker run -it \
  -e BASE64_ENCODED=false \
  -e LOG_LEVEL=DEBUG \
  user-service
 ```

However, to pass the "data" and "params" attributes as JSON-serialized files into the container, you either mount it in the form of separate files (recommended) or pass it as environment variables (base64 encoded).

To use the [`data.json`](input/data.json) and [`params.json`](input/params.json) from the [`input`](input) directory, you could execute the following command:

```bash 
cd app
cd user_code
PROJECT_ROOT=(`pwd`) 
docker run -it \
  -e BASE64_ENCODED=false \
  -e LOG_LEVEL=DEBUG \
  -v $PROJECT_ROOT/input:/var/input \
  user-service
```


* problems : As the build process is started at job-template level, the requirements must also be saved there. However, these come from the user code and must then be moved from there. This makes an additional implementation necessary.


